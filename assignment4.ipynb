{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 2.x\n",
    "\n",
    "1) Подготовка данных\n",
    "\n",
    "2) Использование Keras Model API\n",
    "\n",
    "3) Использование Keras Sequential + Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выполнения лабораторной работы необходимо установить tensorflow версии 2.0 или выше .\n",
    "\n",
    "Рекомендуется использовать возможности Colab'а по обучению моделей на GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных\n",
    "Загрузите набор данных из предыдущей лабораторной работы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Решено было взять mnist с keras, чтобы поинтереснее\n",
    "from keras.datasets import mnist\n",
    "def digits(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "    X_train = X_train.reshape((*X_train.shape, 1))\n",
    "    X_test = X_test.reshape((*X_test.shape, 1))\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    Y_train = np.asarray(Y_train, dtype=np.int32).flatten()\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "    Y_test = np.asarray(Y_test, dtype=np.int32).flatten()\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    Y_val = Y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    Y_train = Y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    Y_test = Y_test[mask]\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
    "\n",
    "\n",
    "train_dset = Dataset(X_train, Y_train, batch_size=64, shuffle=True)\n",
    "val_dset = Dataset(X_val, Y_val, batch_size=64, shuffle=False)\n",
    "test_dset = Dataset(X_test, Y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (64, 28, 28, 1) (64,)\n",
      "1 (64, 28, 28, 1) (64,)\n",
      "2 (64, 28, 28, 1) (64,)\n",
      "3 (64, 28, 28, 1) (64,)\n",
      "4 (64, 28, 28, 1) (64,)\n",
      "5 (64, 28, 28, 1) (64,)\n",
      "6 (64, 28, 28, 1) (64,)\n"
     ]
    }
   ],
   "source": [
    "# We can iterate through a dataset like this:\n",
    "for t, (x, y) in enumerate(train_dset):\n",
    "    print(t, x.shape, y.shape)\n",
    "    if t > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Keras Model Subclassing API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Для реализации собственной модели с помощью Keras Model Subclassing API необходимо выполнить следующие шаги:\n",
    "\n",
    "1) Определить новый класс, который является наследником tf.keras.Model.\n",
    "\n",
    "2) В методе __init__() определить все необходимые слои из модуля tf.keras.layer\n",
    "\n",
    "3) Реализовать прямой проход в методе call() на основе слоев, объявленных в __init__()\n",
    "\n",
    "Ниже приведен пример использования keras API для определения двухслойной полносвязной сети. \n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerFC(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(TwoLayerFC, self).__init__()        \n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def test_TwoLayerFC():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "    x = tf.zeros((64, input_size))\n",
    "    model = TwoLayerFC(hidden_size, num_classes)\n",
    "    #with tf.device(device):\n",
    "    scores = model(x)\n",
    "    print(scores.shape)\n",
    "        \n",
    "test_TwoLayerFC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте трехслойную CNN для вашей задачи классификации. \n",
    "\n",
    "Архитектура сети:\n",
    "    \n",
    "1. Сверточный слой (5 x 5 kernels, zero-padding = 'same')\n",
    "2. Функция активации ReLU \n",
    "3. Сверточный слой (3 x 3 kernels, zero-padding = 'same')\n",
    "4. Функция активации ReLU \n",
    "5. Полносвязный слой \n",
    "6. Функция активации Softmax \n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Conv2D\n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerConvNet(tf.keras.Model):\n",
    "    def __init__(self, channel_1, channel_2, num_classes):\n",
    "        super(ThreeLayerConvNet, self).__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Implement the __init__ method for a three-layer ConvNet. You   #\n",
    "        # should instantiate layer objects to be used in the forward pass.     #\n",
    "        ########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(channel_1, 5, activation=\"relu\", padding=\"same\")\n",
    "        self.conv2 = tf.keras.layers.Conv2D(channel_2, 3, activation=\"relu\", padding=\"same\")\n",
    "        self.fc1 = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                                  )\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward pass for a three-layer ConvNet. You      #\n",
    "        # should use the layer objects defined in the __init__ method.         #\n",
    "        ########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        scores = self.fc1(x)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def test_ThreeLayerConvNet():    \n",
    "    channel_1, channel_2, num_classes = 12, 8, 10\n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    #with tf.device(device):\n",
    "    x = tf.zeros((64, 3, 32, 32))\n",
    "    scores = model(x)\n",
    "    print(scores.shape)\n",
    "\n",
    "test_ThreeLayerConvNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример реализации процесса обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
    "    accuracy on the CIFAR-10 validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints progress during trainingn\n",
    "    \"\"\"    \n",
    "    \n",
    "\n",
    "    \n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    \n",
    "    model = model_init_fn()\n",
    "    optimizer = optimizer_init_fn()\n",
    "    \n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "    \n",
    "    val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "    val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "    \n",
    "    t = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        \n",
    "        for x_np, y_np in train_dset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                \n",
    "                # Use the model function to build the forward pass.\n",
    "                scores = model(x_np, training=is_training)\n",
    "                loss = loss_fn(y_np, scores)\n",
    "    \n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                \n",
    "                # Update the metrics\n",
    "                train_loss.update_state(loss)\n",
    "                train_accuracy.update_state(y_np, scores)\n",
    "                \n",
    "                if t % print_every == 0:\n",
    "                    val_loss.reset_states()\n",
    "                    val_accuracy.reset_states()\n",
    "                    for test_x, test_y in val_dset:\n",
    "                        # During validation at end of epoch, training set to False\n",
    "                        prediction = model(test_x, training=False)\n",
    "                        t_loss = loss_fn(test_y, prediction)\n",
    "\n",
    "                        val_loss.update_state(t_loss)\n",
    "                        val_accuracy.update_state(test_y, prediction)\n",
    "                    \n",
    "                    template = 'Iteration {}, Epoch {}, Loss: {}, Accuracy: {}, Val Loss: {}, Val Accuracy: {}'\n",
    "                    print (template.format(t, epoch+1,\n",
    "                                         train_loss.result(),\n",
    "                                         train_accuracy.result()*100,\n",
    "                                         val_loss.result(),\n",
    "                                         val_accuracy.result()*100))\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.947984218597412, Accuracy: 17.1875, Val Loss: 2.5949792861938477, Val Accuracy: 15.000000953674316\n",
      "Iteration 100, Epoch 1, Loss: 0.660137414932251, Accuracy: 80.43006896972656, Val Loss: 0.5764584541320801, Val Accuracy: 79.9000015258789\n",
      "Iteration 200, Epoch 1, Loss: 0.5209307074546814, Accuracy: 84.57711029052734, Val Loss: 0.4513089060783386, Val Accuracy: 86.19999694824219\n",
      "Iteration 300, Epoch 1, Loss: 0.4622064530849457, Accuracy: 86.30606842041016, Val Loss: 0.42633676528930664, Val Accuracy: 87.19999694824219\n",
      "Iteration 400, Epoch 1, Loss: 0.4166160523891449, Accuracy: 87.62858581542969, Val Loss: 0.37943366169929504, Val Accuracy: 89.60000610351562\n",
      "Iteration 500, Epoch 1, Loss: 0.3917407989501953, Accuracy: 88.35765838623047, Val Loss: 0.3564143180847168, Val Accuracy: 88.80000305175781\n",
      "Iteration 600, Epoch 1, Loss: 0.36796513199806213, Accuracy: 89.05730438232422, Val Loss: 0.3339751958847046, Val Accuracy: 90.20000457763672\n",
      "Iteration 700, Epoch 1, Loss: 0.34946319460868835, Accuracy: 89.65986633300781, Val Loss: 0.3210451304912567, Val Accuracy: 90.5999984741211\n"
     ]
    }
   ],
   "source": [
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    return TwoLayerFC(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите трехслойную CNN. В tf.keras.optimizers.SGD укажите Nesterov momentum = 0.9 . \n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/SGD\n",
    "\n",
    "Значение accuracy на валидационной выборке после 1 эпохи обучения должно быть > 50% ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.3678479194641113, Accuracy: 9.375, Val Loss: 2.326512098312378, Val Accuracy: 7.90000057220459\n",
      "Iteration 100, Epoch 1, Loss: 0.7532694339752197, Accuracy: 79.06868743896484, Val Loss: 0.49550706148147583, Val Accuracy: 83.4000015258789\n",
      "Iteration 200, Epoch 1, Loss: 0.5519068241119385, Accuracy: 84.42941284179688, Val Loss: 0.392255038022995, Val Accuracy: 87.69999694824219\n",
      "Iteration 300, Epoch 1, Loss: 0.44783374667167664, Accuracy: 87.26121520996094, Val Loss: 0.2345581352710724, Val Accuracy: 93.0\n",
      "Iteration 400, Epoch 1, Loss: 0.37554287910461426, Accuracy: 89.2183609008789, Val Loss: 0.21120353043079376, Val Accuracy: 93.0\n",
      "Iteration 500, Epoch 1, Loss: 0.33091387152671814, Accuracy: 90.54078674316406, Val Loss: 0.17576709389686584, Val Accuracy: 94.4000015258789\n",
      "Iteration 600, Epoch 1, Loss: 0.2955970764160156, Accuracy: 91.50114440917969, Val Loss: 0.19803622364997864, Val Accuracy: 93.80000305175781\n",
      "Iteration 700, Epoch 1, Loss: 0.2689714729785919, Accuracy: 92.24099731445312, Val Loss: 0.14117123186588287, Val Accuracy: 95.20000457763672\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "channel_1, channel_2, num_classes = 32, 16, 10\n",
    "\n",
    "def model_init_fn():\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes) \n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использование Keras Sequential API для реализации последовательных моделей.\n",
    "\n",
    "Пример для полносвязной сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 3.1256823539733887, Accuracy: 14.0625, Val Loss: 2.5656204223632812, Val Accuracy: 17.100000381469727\n",
      "Iteration 100, Epoch 1, Loss: 0.6335068941116333, Accuracy: 81.32734680175781, Val Loss: 0.5538548827171326, Val Accuracy: 81.80000305175781\n",
      "Iteration 200, Epoch 1, Loss: 0.5027395486831665, Accuracy: 85.41667175292969, Val Loss: 0.4442679286003113, Val Accuracy: 86.9000015258789\n",
      "Iteration 300, Epoch 1, Loss: 0.4461570978164673, Accuracy: 87.0431900024414, Val Loss: 0.41715800762176514, Val Accuracy: 88.20000457763672\n",
      "Iteration 400, Epoch 1, Loss: 0.4032689034938812, Accuracy: 88.32605743408203, Val Loss: 0.3676564693450928, Val Accuracy: 89.20000457763672\n",
      "Iteration 500, Epoch 1, Loss: 0.3803180754184723, Accuracy: 88.96269989013672, Val Loss: 0.3487617075443268, Val Accuracy: 90.10000610351562\n",
      "Iteration 600, Epoch 1, Loss: 0.3574194014072418, Accuracy: 89.59286499023438, Val Loss: 0.33581745624542236, Val Accuracy: 90.80000305175781\n",
      "Iteration 700, Epoch 1, Loss: 0.340594619512558, Accuracy: 90.1011962890625, Val Loss: 0.31139054894447327, Val Accuracy: 91.19999694824219\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    input_shape = (28, 28, 1)\n",
    "    hidden_layer_size, num_classes = 4000, 10\n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    layers = [\n",
    "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu',\n",
    "                              kernel_initializer=initializer),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax', \n",
    "                              kernel_initializer=initializer),\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернативный менее гибкий способ обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766/766 [==============================] - 13s 17ms/step - loss: 0.3428 - sparse_categorical_accuracy: 0.8988 - val_loss: 0.2892 - val_sparse_categorical_accuracy: 0.9150\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2130 - sparse_categorical_accuracy: 0.9392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21299292147159576, 0.9391999840736389]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_init_fn()\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "model.fit(X_train, Y_train, batch_size=64, epochs=1, validation_data=(X_val, Y_val))\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перепишите реализацию трехслойной CNN с помощью tf.keras.Sequential API . Обучите модель двумя способами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.323291301727295, Accuracy: 10.9375, Val Loss: 2.3317599296569824, Val Accuracy: 6.700000286102295\n",
      "Iteration 100, Epoch 1, Loss: 1.8055198192596436, Accuracy: 52.10396194458008, Val Loss: 0.8948379755020142, Val Accuracy: 75.80000305175781\n",
      "Iteration 200, Epoch 1, Loss: 1.171535849571228, Accuracy: 68.69558715820312, Val Loss: 0.5717899203300476, Val Accuracy: 83.0\n",
      "Iteration 300, Epoch 1, Loss: 0.919281005859375, Accuracy: 75.09862518310547, Val Loss: 0.43958479166030884, Val Accuracy: 87.80000305175781\n",
      "Iteration 400, Epoch 1, Loss: 0.7739182710647583, Accuracy: 78.85364532470703, Val Loss: 0.41879236698150635, Val Accuracy: 87.19999694824219\n",
      "Iteration 500, Epoch 1, Loss: 0.6892250180244446, Accuracy: 81.0691146850586, Val Loss: 0.38131970167160034, Val Accuracy: 89.0999984741211\n",
      "Iteration 600, Epoch 1, Loss: 0.6257354617118835, Accuracy: 82.7059097290039, Val Loss: 0.3902245759963989, Val Accuracy: 88.0\n",
      "Iteration 700, Epoch 1, Loss: 0.5789628624916077, Accuracy: 83.9782485961914, Val Loss: 0.3375300168991089, Val Accuracy: 89.9000015258789\n"
     ]
    }
   ],
   "source": [
    "def model_init_fn():\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a three-layer ConvNet using tf.keras.Sequential.         #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    input_shape = (28,28,1)\n",
    "    channel_1, channel_2, num_classes = 32, 16, 10\n",
    "    layers = [\n",
    "        tf.keras.layers.Conv2D(channel_1, 5, activation=\"relu\", padding=\"same\", input_shape=input_shape),\n",
    "        tf.keras.layers.Conv2D(channel_2, 3, activation=\"relu\", padding=\"same\"),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return model\n",
    "\n",
    "learning_rate = 5e-4\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766/766 [==============================] - 49s 64ms/step - loss: 0.3476 - sparse_categorical_accuracy: 0.8988 - val_loss: 0.2209 - val_sparse_categorical_accuracy: 0.9280\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.1493 - sparse_categorical_accuracy: 0.9590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14928683638572693, 0.9589999914169312]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_init_fn()\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "model.fit(X_train, Y_train, batch_size=64, epochs=1, validation_data=(X_val, Y_val))\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использование Keras Functional API\n",
    "\n",
    "Для реализации более сложных архитектур сети с несколькими входами/выходами, повторным использованием слоев, \"остаточными\" связями (residual connections) необходимо явно указать входные и выходные тензоры. \n",
    "\n",
    "Ниже представлен пример для полносвязной сети. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_functional(input_shape, hidden_size, num_classes):  \n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    flattened_inputs = tf.keras.layers.Flatten()(inputs)\n",
    "    fc1_output = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                 kernel_initializer=initializer)(flattened_inputs)\n",
    "    scores = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                             kernel_initializer=initializer)(fc1_output)\n",
    "\n",
    "    # Instantiate the model given inputs and outputs.\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=scores)\n",
    "    return model\n",
    "\n",
    "def test_two_layer_fc_functional():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "    input_shape = (50,)\n",
    "    \n",
    "    x = tf.zeros((64, input_size))\n",
    "    model = two_layer_fc_functional(input_shape, hidden_size, num_classes)\n",
    "    \n",
    "    scores = model(x)\n",
    "    print(scores.shape)\n",
    "        \n",
    "test_two_layer_fc_functional()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.992885112762451, Accuracy: 10.9375, Val Loss: 2.7037649154663086, Val Accuracy: 17.200000762939453\n",
      "Iteration 100, Epoch 1, Loss: 0.6523125767707825, Accuracy: 80.33724975585938, Val Loss: 0.5814943909645081, Val Accuracy: 81.5999984741211\n",
      "Iteration 200, Epoch 1, Loss: 0.5184507369995117, Accuracy: 84.58488464355469, Val Loss: 0.4533456265926361, Val Accuracy: 86.9000015258789\n",
      "Iteration 300, Epoch 1, Loss: 0.4599704146385193, Accuracy: 86.2853012084961, Val Loss: 0.41271519660949707, Val Accuracy: 87.0\n",
      "Iteration 400, Epoch 1, Loss: 0.41557303071022034, Accuracy: 87.67534637451172, Val Loss: 0.3638101816177368, Val Accuracy: 90.20000457763672\n",
      "Iteration 500, Epoch 1, Loss: 0.39124810695648193, Accuracy: 88.43251037597656, Val Loss: 0.35531482100486755, Val Accuracy: 90.5999984741211\n",
      "Iteration 600, Epoch 1, Loss: 0.3677675426006317, Accuracy: 89.09109497070312, Val Loss: 0.3328782618045807, Val Accuracy: 91.0999984741211\n",
      "Iteration 700, Epoch 1, Loss: 0.3499121069908142, Accuracy: 89.65540313720703, Val Loss: 0.3130183517932892, Val Accuracy: 91.19999694824219\n"
     ]
    }
   ],
   "source": [
    "input_shape = (28, 28, 1)\n",
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    return two_layer_fc_functional(input_shape, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэкспериментируйте с архитектурой сверточной сети. Для вашего набора данных вам необходимо получить как минимум 70% accuracy на валидационной выборке за 10 эпох обучения. Опишите все эксперименты и сделайте выводы (без выполнения данного пункта работы приниматься не будут). \n",
    "\n",
    "Эспериментируйте с архитектурой, гиперпараметрами, функцией потерь, регуляризацией, методом оптимизации.  \n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization#methods https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dropout#methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишите все эксперименты, результаты. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.4646525382995605, Accuracy: 7.8125, Val Loss: 3.530057191848755, Val Accuracy: 22.5\n",
      "Iteration 700, Epoch 1, Loss: 0.1361669898033142, Accuracy: 95.87419891357422, Val Loss: 0.09605340659618378, Val Accuracy: 97.5999984741211\n",
      "Iteration 1400, Epoch 2, Loss: 0.045632120221853256, Accuracy: 98.57037353515625, Val Loss: 0.0678844228386879, Val Accuracy: 97.69999694824219\n",
      "Iteration 2100, Epoch 3, Loss: 0.02690119855105877, Accuracy: 99.11576843261719, Val Loss: 0.07001487165689468, Val Accuracy: 97.89999389648438\n",
      "Iteration 2800, Epoch 4, Loss: 0.0223290603607893, Accuracy: 99.2513656616211, Val Loss: 0.04552684724330902, Val Accuracy: 98.5\n",
      "Iteration 3500, Epoch 5, Loss: 0.015188204124569893, Accuracy: 99.4708251953125, Val Loss: 0.06064394861459732, Val Accuracy: 98.5999984741211\n",
      "Iteration 4200, Epoch 6, Loss: 0.01406104862689972, Accuracy: 99.51145935058594, Val Loss: 0.09125300496816635, Val Accuracy: 97.5999984741211\n",
      "Iteration 4900, Epoch 7, Loss: 0.009468834847211838, Accuracy: 99.68238067626953, Val Loss: 0.0871821716427803, Val Accuracy: 97.5999984741211\n",
      "Iteration 5600, Epoch 8, Loss: 0.010180308483541012, Accuracy: 99.69927215576172, Val Loss: 0.10501763224601746, Val Accuracy: 98.4000015258789\n",
      "Iteration 6300, Epoch 9, Loss: 0.013032747432589531, Accuracy: 99.58454132080078, Val Loss: 0.09535779058933258, Val Accuracy: 97.5\n",
      "Iteration 7000, Epoch 10, Loss: 0.007111677434295416, Accuracy: 99.73715209960938, Val Loss: 0.10551850497722626, Val Accuracy: 97.69999694824219\n"
     ]
    }
   ],
   "source": [
    "class CustomConvNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomConvNet, self).__init__()\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        self.conv1 = tf.keras.layers.Conv2D(channel_1, 5, activation=\"relu\", padding=\"same\")\n",
    "        self.mp1 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='valid')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(channel_2, 3, activation=\"relu\", padding=\"same\")\n",
    "        self.mp2 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='valid')\n",
    "        self.fc1=tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        \n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "    \n",
    "    def call(self, input_tensor, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.mp1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.mp2(x)\n",
    "        \n",
    "        x=self.flatten(x)\n",
    "        x=self.fc1(x)\n",
    "        x=self.fc2(x)\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        return x\n",
    "\n",
    "\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "\n",
    "model = CustomConvNet()\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomConvNet()\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 7.5e-4\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотелось бы отметить, что сети показывают хороший результат, идет уже борьба за лучшую точность. Как правило, 10 эпох никогда не требуется, и значения accuracy начинают колебаться просто то в одну, то в другую сторону, так что итоги будут смотреть по лучшим значениям, а не только в конце 10 эпохи\n",
    "\n",
    "1) Для начала к трехслойной сети были добавлены слои maxpooling - ничего интересного\n",
    "\n",
    "2) Добавим два блока Conv2d, затем maxpool, затем повторим - обучаться стало куда тяжелее, результат особо лучше не стал\n",
    "\n",
    "3) Поставим увеличение карт - 16, 32, 64 - Результат примерно такой же, но насколько же дольше мы стали обучаться\n",
    "\n",
    "4) Сразу несколько изменений. Все ядра сверток фиксированный размер 3 на 3, maxpooling  с valid, еще один полносвязный слой на 256 нейронов, уменьшили темп обучения, оставили только по одной свертке на уровень - на пятой эпохе 98.6 и это лучший результат, что удалось получить(он в ячейке)\n",
    "\n",
    "5) Попутно введем дропаут и батчнорм(просто ради интереса, ничем не обоснованно) - стало хуже, но занимательно, что при дропауте после последней свертки значения accuracy на val, было куда выше, чем на train.\n",
    "\n",
    "6) Добавляем еще два полносвязных слоя - и все стало хуже, точность около 93, явное переобучение.\n",
    "\n",
    "\n",
    "Сеть в ячейке лучшая с достигнутой accuracy на val(5 эпоха), и лучшая с точки зрения вычисления/результаты. \n",
    "\n",
    "Для остановки на лучшей эпохе, можно использовать keras.callback.earlystopping, но в текущей реализации его не очень удобно прикрутить, так что отслеживание на глазок\n",
    "\n",
    "Еще раз хотелось бы подчеркнуть, что абсолютно все модели выдавали 97% на val(на лучших эпохах)(кроме 5) и соревнование шло за лучшую точность\n",
    "\n",
    "Судя по kaggle,  рецепт лучшей accuracy - агументация данных, по две свертки одинаково размера, после которых следует maxpool,  и батчнорм, число фильтров 64, 128б 256,  полносвязный слой на 512 нейронов, 50 эпох - дают 99+ на val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
